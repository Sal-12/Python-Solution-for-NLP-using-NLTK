{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('British', 'NOUN'),\n",
       " ('left', 'ADVERB'),\n",
       " ('Waffles', 'PLURAL'),\n",
       " ('on', 'VERB'),\n",
       " ('Falkland', 'NOUN'),\n",
       " ('islands', 'PLURAL')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoof_headline = 'British/NOUN left/ADVERB Waffles/Plural on/VERB Falkland/NOUN islands/PLURAL'\n",
    "[nltk.tag.str2tuple(h) for h in spoof_headline.split()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NOUN', 'VERB'])\n",
      "5\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "brown_words = brown.tagged_words(tagset='universal')\n",
    "tag_Freq = nltk.FreqDist(tag for (word, tag) in brown_words if word == 'color')\n",
    "print(tag_Freq.keys())\n",
    "print(tag_Freq['VERB'])\n",
    "print(tag_Freq['NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NOUN', 'VERB'])\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "tag_Freq = nltk.FreqDist(tag for (word, tag) in brown_words if word == 'fade')\n",
    "print(tag_Freq.keys())\n",
    "print(tag_Freq['NOUN'])\n",
    "print(tag_Freq['VERB'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('wind', 'VBP'),\n",
       " ('back', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('clock', 'NN'),\n",
       " (',', ','),\n",
       " ('while', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('chase', 'VBP'),\n",
       " ('after', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('wind', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = 'They wind back the clock, while we chase after the wind.'\n",
    "nltk.pos_tag(nltk.word_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# linguistics Objects Mappings from Keys to Values:\n",
    "# the structure of words, syntax, phonetics transcriptions etc. It takes a linguistic object matches it to the map of keys to values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc': 'ALPHA', '123': 'BETA'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "d['abc'] = 'ALPHA'\n",
    "d['123'] = 'BETA'\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d['xyz']  # we will have an error as d['xyz'] is non existent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'BETA'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del d['abc']\n",
    "d               #del deletes the item for the list "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Python': 1,\n",
       " 'With': 2,\n",
       " 'NLP': 3,\n",
       " 'Is': 4,\n",
       " 'FUN': 5,\n",
       " 'NLTK': 0,\n",
       " 'Scapy': 7,\n",
       " 'Pytorch': 9,\n",
       " 'Tensorflow': 89}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = {'Python': 1, 'With':2 , 'NLP':3, 'Is':4, 'FUN':5}\n",
    "d2 = {'NLTK': 0, 'Scapy':7, 'Pytorch':9, 'Tensorflow':89}\n",
    "d1.update(d2)            # both the dictionaries where added to d1.# Useful for combining/merging dictionaires.\n",
    "d1                                                     "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headword': 'harry potter',\n",
       " 'part-of-speech': 'naruto talk no jutsu',\n",
       " 'sense': 'too many movies and shows',\n",
       " 'example': 'Anime and fantasy are an amazing combination'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = {'headword': 'harry potter', 'part-of-speech': 'naruto talk no jutsu', 'sense': 'too many movies and shows', \n",
    "    'example': 'Anime and fantasy are an amazing combination'}\n",
    "e                           # This was just for FUN!!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# GO is sort of like a possibility for future.\n",
    "# WENT is like the thing has already happened in the past. thus it would not possible to use them interchangbly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 507 matches:\n",
      "ade good his promise . `` Everything went real smooth '' , the sheriff said . \n",
      "axation . Under committee rules , it went automatically to a subcommittee for \n",
      "e . And after several correspondents went into Pathet Lao territory and expose\n",
      "the Kansas City scoring in the sixth went like this : Lumpe worked a walk as t\n",
      "his season to 13 straight before one went astray last Saturday night in the 41\n",
      "nd caught one pass for 13 yards . He went into the Army in March , 1957 , and \n",
      "m Monday , ran for 30 minutes , then went in , while the reserves scrimmaged f\n",
      "tchie of the Ogden Standard Examiner went to his compartment to talk with him \n",
      "pped into his second shot . The ball went off in a majestic arc , an out-of-bo\n",
      " rare sense of humor . Everywhere he went in town , people sidled up , gave hi\n",
      "e is where a man was born , reared , went to school and , most particularly , \n",
      " off for good behavior . He promptly went to communist East Germany . The magi\n",
      "in the Skipjack . With the machinery went a complete design for the hull . The\n",
      "issile submarines '' , the statement went on . The atom reactor , water cooled\n",
      "e this account : Thomas early Sunday went to the home of his uncle and aunt , \n",
      "sheep showman contest . Blue ribbons went to Stephanie Shaw of Hillsboro , Lar\n",
      "nks . Swine showmanship championship went to Bob Day , with Tom Day and Hutchi\n",
      "d on the jury . Then , when the case went to the jury , the judge excused one \n",
      "streamer of dust it landed . Fiedler went on to make several other test flight\n",
      "inals . Second grand prize of $5,000 went to Mrs. Clara L. Oliver for her Hawa\n",
      "Last two to be added before the book went to press were the marriages of Mered\n",
      " next year's television shows . So I went to see `` La Dolce Vita '' . It has \n",
      "ou have language problems . The week went along briskly enough . I bought a ne\n",
      "eaded up one of the four groups that went on simultaneous tours after the Gall\n",
      "tings and sculptures to each group , went one of the Gallery's blue-uniformed \n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "Text(brown.words()).concordance('went')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 626 matches:\n",
      "struction bonds . The bond issue will go to the state courts for a friendly te\n",
      "ress text still had `` quite a way to go '' toward completion . Decisions are \n",
      " replied , `` I would say it's got to go thru several more drafts '' . Salinge\n",
      "ause the levy is already scheduled to go up by 1 per cent on that date to pay \n",
      "irst year , 1963 . Both figures would go higher in later years . Other parts o\n",
      "lion dollars the first year and would go up to 21 millions by 1966 . The Presi\n",
      "said yesterday he would be willing to go before the city council `` or anyone \n",
      "red would know what to do or where to go in the event of an enemy attack . The\n",
      "e another . Riverside residents would go to the Seekonk assembly point . Mr. H\n",
      "nk E. Smith as the one most likely to go , the redistricting battle will put t\n",
      "e battle . Then he could tell them to go home , while the administration conti\n",
      "pinion as to how far the board should go , and whose advice it should follow .\n",
      "n to where the parents wanted them to go . Dr. Melvin W. Barnes , superintende\n",
      "ay night in Salem . On Friday he will go to Portland for the swearing in of De\n",
      "`` Then we'd really have someplace to go '' . Bowie , Md. , March 17 -- Gainin\n",
      "`` Spahnie doesn't know how to merely go through the motions '' , remarked Eno\n",
      "Slocum Memorial Award . To Spahn will go the Sid Mercer Memorial Award as the \n",
      "ds . The writers' Gold Tee Award will go to John McAuliffe of Plainfield , N. \n",
      "ever , Mr. Parichy and his bride will go to Vero Beach on their wedding trip ,\n",
      "ncy '' . No matter how many Americans go abroad in summer , probably a hundred\n",
      ", service and comfort stations ( they go together like Scots and heather ) , d\n",
      "Werner said , was let manual laborers go home Tuesday night for some rest . Wo\n",
      "t destroyers . It could , reputedly , go 70,000 miles without refueling and st\n",
      "dinator of audio-visual education may go to the state Supreme Court , it appea\n",
      " therefore knew them . It was time to go up myself '' . Fiedler was then techn\n"
     ]
    }
   ],
   "source": [
    "Text(brown.words()).concordance('go')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP', None),\n",
       " ('with', 'IN'),\n",
       " ('python', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('fun', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('beginning', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('later', 'RBR'),\n",
       " ('on', 'IN'),\n",
       " ('it', 'PPS'),\n",
       " ('just', 'RB'),\n",
       " ('gets', 'VBZ'),\n",
       " ('a', 'AT'),\n",
       " ('little', 'AP'),\n",
       " ('tedious', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('However', 'RB'),\n",
       " ('I', 'PPSS'),\n",
       " ('do', 'DO'),\n",
       " ('Enjoy', 'VB'),\n",
       " ('learning', 'VBG'),\n",
       " ('it', 'PPS'),\n",
       " ('on', 'IN'),\n",
       " ('my', 'PP$'),\n",
       " ('own', 'JJ'),\n",
       " ('with', 'IN'),\n",
       " ('online', None),\n",
       " ('available', 'JJ'),\n",
       " ('resources', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents()\n",
    "unigram_tags = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tags.tag(nltk.word_tokenize('''NLP with python is fun in the beginning but later on it just gets a little tedious.\n",
    "                                           However I do Enjoy learning it on my own with online available resources.'''))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AffixTagger in module nltk.tag.sequential:\n",
      "\n",
      "class AffixTagger(ContextTagger)\n",
      " |  AffixTagger(train=None, model=None, affix_length=-3, min_stem_length=2, backoff=None, cutoff=0, verbose=False)\n",
      " |  \n",
      " |  A tagger that chooses a token's tag based on a leading or trailing\n",
      " |  substring of its word string.  (It is important to note that these\n",
      " |  substrings are not necessarily \"true\" morphological affixes).  In\n",
      " |  particular, a fixed-length substring of the word is looked up in a\n",
      " |  table, and the corresponding tag is returned.  Affix taggers are\n",
      " |  typically constructed by training them on a tagged corpus.\n",
      " |  \n",
      " |  Construct a new affix tagger.\n",
      " |  \n",
      " |  :param affix_length: The length of the affixes that should be\n",
      " |      considered during training and tagging.  Use negative\n",
      " |      numbers for suffixes.\n",
      " |  :param min_stem_length: Any words whose length is less than\n",
      " |      min_stem_length+abs(affix_length) will be assigned a\n",
      " |      tag of None by this tagger.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AffixTagger\n",
      " |      ContextTagger\n",
      " |      SequentialBackoffTagger\n",
      " |      nltk.tag.api.TaggerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, train=None, model=None, affix_length=-3, min_stem_length=2, backoff=None, cutoff=0, verbose=False)\n",
      " |      :param context_to_tag: A dictionary mapping contexts to tags.\n",
      " |      :param backoff: The backoff tagger that should be used for this tagger.\n",
      " |  \n",
      " |  context(self, tokens, index, history)\n",
      " |      :return: the context that should be used to look up the tag\n",
      " |          for the specified token; or None if the specified token\n",
      " |          should not be handled by this tagger.\n",
      " |      :rtype: (hashable)\n",
      " |  \n",
      " |  encode_json_obj(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  decode_json_obj(obj) from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  json_tag = 'nltk.tag.sequential.AffixTagger'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ContextTagger:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |  \n",
      " |  choose_tag(self, tokens, index, history)\n",
      " |      Decide which tag should be used for the specified token, and\n",
      " |      return that tag.  If this tagger is unable to determine a tag\n",
      " |      for the specified token, return None -- do not consult\n",
      " |      the backoff tagger.  This method should be overridden by\n",
      " |      subclasses of SequentialBackoffTagger.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |      :type tokens: list\n",
      " |      :param tokens: The list of words that are being tagged.\n",
      " |      :type index: int\n",
      " |      :param index: The index of the word whose tag should be\n",
      " |          returned.\n",
      " |      :type history: list(str)\n",
      " |      :param history: A list of the tags for all words before *index*.\n",
      " |  \n",
      " |  size(self)\n",
      " |      :return: The number of entries in the table used by this\n",
      " |          tagger to map from contexts to tags.\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from SequentialBackoffTagger:\n",
      " |  \n",
      " |  tag(self, tokens)\n",
      " |      Determine the most appropriate tag sequence for the given\n",
      " |      token sequence, and return a corresponding list of tagged\n",
      " |      tokens.  A tagged token is encoded as a tuple ``(token, tag)``.\n",
      " |      \n",
      " |      :rtype: list(tuple(str, str))\n",
      " |  \n",
      " |  tag_one(self, tokens, index, history)\n",
      " |      Determine an appropriate tag for the specified token, and\n",
      " |      return that tag.  If this tagger is unable to determine a tag\n",
      " |      for the specified token, then its backoff tagger is consulted.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |      :type tokens: list\n",
      " |      :param tokens: The list of words that are being tagged.\n",
      " |      :type index: int\n",
      " |      :param index: The index of the word whose tag should be\n",
      " |          returned.\n",
      " |      :type history: list(str)\n",
      " |      :param history: A list of the tags for all words before *index*.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SequentialBackoffTagger:\n",
      " |  \n",
      " |  backoff\n",
      " |      The backoff tagger for this tagger.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  evaluate(self, gold)\n",
      " |      Score the accuracy of the tagger against the gold standard.\n",
      " |      Strip the tags from the gold standard text, retag it using\n",
      " |      the tagger, then compute the accuracy score.\n",
      " |      \n",
      " |      :type gold: list(list(tuple(str, str)))\n",
      " |      :param gold: The list of tagged sentences to score the tagger on.\n",
      " |      :rtype: float\n",
      " |  \n",
      " |  tag_sents(self, sentences)\n",
      " |      Apply ``self.tag()`` to each element of *sentences*.  I.e.:\n",
      " |      \n",
      " |          return [self.tag(sent) for sent in sentences]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.AffixTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP', None),\n",
       " ('with', None),\n",
       " ('python', 'NN'),\n",
       " ('and', None),\n",
       " ('scapy', 'NN'),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affix_tagger = nltk.AffixTagger(brown_tagged_sents, affix_length=2, min_stem_length=3)\n",
    "affix_tagger.tag(nltk.word_tokenize('NLP with python and scapy.'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBN'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'WPS'),\n",
       " ('any', None),\n",
       " ('irregularities', None),\n",
       " ('took', None),\n",
       " ('place', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.tag(brown.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP', None),\n",
       " ('with', None),\n",
       " ('python', None),\n",
       " ('and', None),\n",
       " ('scapy', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.tag(nltk.word_tokenize('NLP with python and scapy.'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# since the data given is unseen it displays the value of 'NOUN' on every entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-12-21'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21/12/2019'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.today().strftime(\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.2019.21'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.today().strftime(\"%m.%Y.%d\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " \"''\",\n",
       " '(',\n",
       " '(-HL',\n",
       " ')',\n",
       " ')-HL',\n",
       " '*',\n",
       " '*-HL',\n",
       " '*-NC',\n",
       " '*-TL',\n",
       " ',',\n",
       " ',-HL',\n",
       " ',-NC',\n",
       " ',-TL',\n",
       " '--',\n",
       " '---HL',\n",
       " '.',\n",
       " '.-HL',\n",
       " '.-NC',\n",
       " '.-TL',\n",
       " ':',\n",
       " ':-HL',\n",
       " ':-TL',\n",
       " 'ABL',\n",
       " 'ABN',\n",
       " 'ABN-HL',\n",
       " 'ABN-NC',\n",
       " 'ABN-TL',\n",
       " 'ABX',\n",
       " 'AP',\n",
       " 'AP$',\n",
       " 'AP+AP-NC',\n",
       " 'AP-HL',\n",
       " 'AP-NC',\n",
       " 'AP-TL',\n",
       " 'AT',\n",
       " 'AT-HL',\n",
       " 'AT-NC',\n",
       " 'AT-TL',\n",
       " 'AT-TL-HL',\n",
       " 'BE',\n",
       " 'BE-HL',\n",
       " 'BE-TL',\n",
       " 'BED',\n",
       " 'BED*',\n",
       " 'BED-NC',\n",
       " 'BEDZ',\n",
       " 'BEDZ*',\n",
       " 'BEDZ-HL',\n",
       " 'BEDZ-NC',\n",
       " 'BEG',\n",
       " 'BEM',\n",
       " 'BEM*',\n",
       " 'BEM-NC',\n",
       " 'BEN',\n",
       " 'BEN-TL',\n",
       " 'BER',\n",
       " 'BER*',\n",
       " 'BER*-NC',\n",
       " 'BER-HL',\n",
       " 'BER-NC',\n",
       " 'BER-TL',\n",
       " 'BEZ',\n",
       " 'BEZ*',\n",
       " 'BEZ-HL',\n",
       " 'BEZ-NC',\n",
       " 'BEZ-TL',\n",
       " 'CC',\n",
       " 'CC-HL',\n",
       " 'CC-NC',\n",
       " 'CC-TL',\n",
       " 'CC-TL-HL',\n",
       " 'CD',\n",
       " 'CD$',\n",
       " 'CD-HL',\n",
       " 'CD-NC',\n",
       " 'CD-TL',\n",
       " 'CD-TL-HL',\n",
       " 'CS',\n",
       " 'CS-HL',\n",
       " 'CS-NC',\n",
       " 'CS-TL',\n",
       " 'DO',\n",
       " 'DO*',\n",
       " 'DO*-HL',\n",
       " 'DO+PPSS',\n",
       " 'DO-HL',\n",
       " 'DO-NC',\n",
       " 'DO-TL',\n",
       " 'DOD',\n",
       " 'DOD*',\n",
       " 'DOD*-TL',\n",
       " 'DOD-NC',\n",
       " 'DOZ',\n",
       " 'DOZ*',\n",
       " 'DOZ*-TL',\n",
       " 'DOZ-HL',\n",
       " 'DOZ-TL',\n",
       " 'DT',\n",
       " 'DT$',\n",
       " 'DT+BEZ',\n",
       " 'DT+BEZ-NC',\n",
       " 'DT+MD',\n",
       " 'DT-HL',\n",
       " 'DT-NC',\n",
       " 'DT-TL',\n",
       " 'DTI',\n",
       " 'DTI-HL',\n",
       " 'DTI-TL',\n",
       " 'DTS',\n",
       " 'DTS+BEZ',\n",
       " 'DTS-HL',\n",
       " 'DTX',\n",
       " 'EX',\n",
       " 'EX+BEZ',\n",
       " 'EX+HVD',\n",
       " 'EX+HVZ',\n",
       " 'EX+MD',\n",
       " 'EX-HL',\n",
       " 'EX-NC',\n",
       " 'FW-*',\n",
       " 'FW-*-TL',\n",
       " 'FW-AT',\n",
       " 'FW-AT+NN-TL',\n",
       " 'FW-AT+NP-TL',\n",
       " 'FW-AT-HL',\n",
       " 'FW-AT-TL',\n",
       " 'FW-BE',\n",
       " 'FW-BER',\n",
       " 'FW-BEZ',\n",
       " 'FW-CC',\n",
       " 'FW-CC-TL',\n",
       " 'FW-CD',\n",
       " 'FW-CD-TL',\n",
       " 'FW-CS',\n",
       " 'FW-DT',\n",
       " 'FW-DT+BEZ',\n",
       " 'FW-DTS',\n",
       " 'FW-HV',\n",
       " 'FW-IN',\n",
       " 'FW-IN+AT',\n",
       " 'FW-IN+AT-T',\n",
       " 'FW-IN+AT-TL',\n",
       " 'FW-IN+NN',\n",
       " 'FW-IN+NN-TL',\n",
       " 'FW-IN+NP-TL',\n",
       " 'FW-IN-TL',\n",
       " 'FW-JJ',\n",
       " 'FW-JJ-NC',\n",
       " 'FW-JJ-TL',\n",
       " 'FW-JJR',\n",
       " 'FW-JJT',\n",
       " 'FW-NN',\n",
       " 'FW-NN$',\n",
       " 'FW-NN$-TL',\n",
       " 'FW-NN-NC',\n",
       " 'FW-NN-TL',\n",
       " 'FW-NN-TL-NC',\n",
       " 'FW-NNS',\n",
       " 'FW-NNS-NC',\n",
       " 'FW-NNS-TL',\n",
       " 'FW-NP',\n",
       " 'FW-NP-TL',\n",
       " 'FW-NPS',\n",
       " 'FW-NPS-TL',\n",
       " 'FW-NR',\n",
       " 'FW-NR-TL',\n",
       " 'FW-OD-NC',\n",
       " 'FW-OD-TL',\n",
       " 'FW-PN',\n",
       " 'FW-PP$',\n",
       " 'FW-PP$-NC',\n",
       " 'FW-PP$-TL',\n",
       " 'FW-PPL',\n",
       " 'FW-PPL+VBZ',\n",
       " 'FW-PPO',\n",
       " 'FW-PPO+IN',\n",
       " 'FW-PPS',\n",
       " 'FW-PPSS',\n",
       " 'FW-PPSS+HV',\n",
       " 'FW-QL',\n",
       " 'FW-RB',\n",
       " 'FW-RB+CC',\n",
       " 'FW-RB-TL',\n",
       " 'FW-TO+VB',\n",
       " 'FW-UH',\n",
       " 'FW-UH-NC',\n",
       " 'FW-UH-TL',\n",
       " 'FW-VB',\n",
       " 'FW-VB-NC',\n",
       " 'FW-VB-TL',\n",
       " 'FW-VBD',\n",
       " 'FW-VBD-TL',\n",
       " 'FW-VBG',\n",
       " 'FW-VBG-TL',\n",
       " 'FW-VBN',\n",
       " 'FW-VBZ',\n",
       " 'FW-WDT',\n",
       " 'FW-WPO',\n",
       " 'FW-WPS',\n",
       " 'HV',\n",
       " 'HV*',\n",
       " 'HV+TO',\n",
       " 'HV-HL',\n",
       " 'HV-NC',\n",
       " 'HV-TL',\n",
       " 'HVD',\n",
       " 'HVD*',\n",
       " 'HVD-HL',\n",
       " 'HVG',\n",
       " 'HVG-HL',\n",
       " 'HVN',\n",
       " 'HVZ',\n",
       " 'HVZ*',\n",
       " 'HVZ-NC',\n",
       " 'HVZ-TL',\n",
       " 'IN',\n",
       " 'IN+IN',\n",
       " 'IN+PPO',\n",
       " 'IN-HL',\n",
       " 'IN-NC',\n",
       " 'IN-TL',\n",
       " 'IN-TL-HL',\n",
       " 'JJ',\n",
       " 'JJ$-TL',\n",
       " 'JJ+JJ-NC',\n",
       " 'JJ-HL',\n",
       " 'JJ-NC',\n",
       " 'JJ-TL',\n",
       " 'JJ-TL-HL',\n",
       " 'JJ-TL-NC',\n",
       " 'JJR',\n",
       " 'JJR+CS',\n",
       " 'JJR-HL',\n",
       " 'JJR-NC',\n",
       " 'JJR-TL',\n",
       " 'JJS',\n",
       " 'JJS-HL',\n",
       " 'JJS-TL',\n",
       " 'JJT',\n",
       " 'JJT-HL',\n",
       " 'JJT-NC',\n",
       " 'JJT-TL',\n",
       " 'MD',\n",
       " 'MD*',\n",
       " 'MD*-HL',\n",
       " 'MD+HV',\n",
       " 'MD+PPSS',\n",
       " 'MD+TO',\n",
       " 'MD-HL',\n",
       " 'MD-NC',\n",
       " 'MD-TL',\n",
       " 'NIL',\n",
       " 'NN',\n",
       " 'NN$',\n",
       " 'NN$-HL',\n",
       " 'NN$-TL',\n",
       " 'NN+BEZ',\n",
       " 'NN+BEZ-TL',\n",
       " 'NN+HVD-TL',\n",
       " 'NN+HVZ',\n",
       " 'NN+HVZ-TL',\n",
       " 'NN+IN',\n",
       " 'NN+MD',\n",
       " 'NN+NN-NC',\n",
       " 'NN-HL',\n",
       " 'NN-NC',\n",
       " 'NN-TL',\n",
       " 'NN-TL-HL',\n",
       " 'NN-TL-NC',\n",
       " 'NNS',\n",
       " 'NNS$',\n",
       " 'NNS$-HL',\n",
       " 'NNS$-NC',\n",
       " 'NNS$-TL',\n",
       " 'NNS$-TL-HL',\n",
       " 'NNS+MD',\n",
       " 'NNS-HL',\n",
       " 'NNS-NC',\n",
       " 'NNS-TL',\n",
       " 'NNS-TL-HL',\n",
       " 'NNS-TL-NC',\n",
       " 'NP',\n",
       " 'NP$',\n",
       " 'NP$-HL',\n",
       " 'NP$-TL',\n",
       " 'NP+BEZ',\n",
       " 'NP+BEZ-NC',\n",
       " 'NP+HVZ',\n",
       " 'NP+HVZ-NC',\n",
       " 'NP+MD',\n",
       " 'NP-HL',\n",
       " 'NP-NC',\n",
       " 'NP-TL',\n",
       " 'NP-TL-HL',\n",
       " 'NPS',\n",
       " 'NPS$',\n",
       " 'NPS$-HL',\n",
       " 'NPS$-TL',\n",
       " 'NPS-HL',\n",
       " 'NPS-NC',\n",
       " 'NPS-TL',\n",
       " 'NR',\n",
       " 'NR$',\n",
       " 'NR$-TL',\n",
       " 'NR+MD',\n",
       " 'NR-HL',\n",
       " 'NR-NC',\n",
       " 'NR-TL',\n",
       " 'NR-TL-HL',\n",
       " 'NRS',\n",
       " 'NRS-TL',\n",
       " 'OD',\n",
       " 'OD-HL',\n",
       " 'OD-NC',\n",
       " 'OD-TL',\n",
       " 'PN',\n",
       " 'PN$',\n",
       " 'PN+BEZ',\n",
       " 'PN+HVD',\n",
       " 'PN+HVZ',\n",
       " 'PN+MD',\n",
       " 'PN-HL',\n",
       " 'PN-NC',\n",
       " 'PN-TL',\n",
       " 'PP$',\n",
       " 'PP$$',\n",
       " 'PP$-HL',\n",
       " 'PP$-NC',\n",
       " 'PP$-TL',\n",
       " 'PPL',\n",
       " 'PPL-HL',\n",
       " 'PPL-NC',\n",
       " 'PPL-TL',\n",
       " 'PPLS',\n",
       " 'PPO',\n",
       " 'PPO-HL',\n",
       " 'PPO-NC',\n",
       " 'PPO-TL',\n",
       " 'PPS',\n",
       " 'PPS+BEZ',\n",
       " 'PPS+BEZ-HL',\n",
       " 'PPS+BEZ-NC',\n",
       " 'PPS+HVD',\n",
       " 'PPS+HVZ',\n",
       " 'PPS+MD',\n",
       " 'PPS-HL',\n",
       " 'PPS-NC',\n",
       " 'PPS-TL',\n",
       " 'PPSS',\n",
       " 'PPSS+BEM',\n",
       " 'PPSS+BER',\n",
       " 'PPSS+BER-N',\n",
       " 'PPSS+BER-NC',\n",
       " 'PPSS+BER-TL',\n",
       " 'PPSS+BEZ',\n",
       " 'PPSS+BEZ*',\n",
       " 'PPSS+HV',\n",
       " 'PPSS+HV-TL',\n",
       " 'PPSS+HVD',\n",
       " 'PPSS+MD',\n",
       " 'PPSS+MD-NC',\n",
       " 'PPSS+VB',\n",
       " 'PPSS-HL',\n",
       " 'PPSS-NC',\n",
       " 'PPSS-TL',\n",
       " 'QL',\n",
       " 'QL-HL',\n",
       " 'QL-NC',\n",
       " 'QL-TL',\n",
       " 'QLP',\n",
       " 'RB',\n",
       " 'RB$',\n",
       " 'RB+BEZ',\n",
       " 'RB+BEZ-HL',\n",
       " 'RB+BEZ-NC',\n",
       " 'RB+CS',\n",
       " 'RB-HL',\n",
       " 'RB-NC',\n",
       " 'RB-TL',\n",
       " 'RBR',\n",
       " 'RBR+CS',\n",
       " 'RBR-NC',\n",
       " 'RBT',\n",
       " 'RN',\n",
       " 'RP',\n",
       " 'RP+IN',\n",
       " 'RP-HL',\n",
       " 'RP-NC',\n",
       " 'RP-TL',\n",
       " 'TO',\n",
       " 'TO+VB',\n",
       " 'TO-HL',\n",
       " 'TO-NC',\n",
       " 'TO-TL',\n",
       " 'UH',\n",
       " 'UH-HL',\n",
       " 'UH-NC',\n",
       " 'UH-TL',\n",
       " 'VB',\n",
       " 'VB+AT',\n",
       " 'VB+IN',\n",
       " 'VB+JJ-NC',\n",
       " 'VB+PPO',\n",
       " 'VB+RP',\n",
       " 'VB+TO',\n",
       " 'VB+VB-NC',\n",
       " 'VB-HL',\n",
       " 'VB-NC',\n",
       " 'VB-TL',\n",
       " 'VBD',\n",
       " 'VBD-HL',\n",
       " 'VBD-NC',\n",
       " 'VBD-TL',\n",
       " 'VBG',\n",
       " 'VBG+TO',\n",
       " 'VBG-HL',\n",
       " 'VBG-NC',\n",
       " 'VBG-TL',\n",
       " 'VBN',\n",
       " 'VBN+TO',\n",
       " 'VBN-HL',\n",
       " 'VBN-NC',\n",
       " 'VBN-TL',\n",
       " 'VBN-TL-HL',\n",
       " 'VBN-TL-NC',\n",
       " 'VBZ',\n",
       " 'VBZ-HL',\n",
       " 'VBZ-NC',\n",
       " 'VBZ-TL',\n",
       " 'WDT',\n",
       " 'WDT+BER',\n",
       " 'WDT+BER+PP',\n",
       " 'WDT+BEZ',\n",
       " 'WDT+BEZ-HL',\n",
       " 'WDT+BEZ-NC',\n",
       " 'WDT+BEZ-TL',\n",
       " 'WDT+DO+PPS',\n",
       " 'WDT+DOD',\n",
       " 'WDT+HVZ',\n",
       " 'WDT-HL',\n",
       " 'WDT-NC',\n",
       " 'WP$',\n",
       " 'WPO',\n",
       " 'WPO-NC',\n",
       " 'WPO-TL',\n",
       " 'WPS',\n",
       " 'WPS+BEZ',\n",
       " 'WPS+BEZ-NC',\n",
       " 'WPS+BEZ-TL',\n",
       " 'WPS+HVD',\n",
       " 'WPS+HVZ',\n",
       " 'WPS+MD',\n",
       " 'WPS-HL',\n",
       " 'WPS-NC',\n",
       " 'WPS-TL',\n",
       " 'WQL',\n",
       " 'WQL-TL',\n",
       " 'WRB',\n",
       " 'WRB+BER',\n",
       " 'WRB+BEZ',\n",
       " 'WRB+BEZ-TL',\n",
       " 'WRB+DO',\n",
       " 'WRB+DOD',\n",
       " 'WRB+DOD*',\n",
       " 'WRB+DOZ',\n",
       " 'WRB+IN',\n",
       " 'WRB+MD',\n",
       " 'WRB-HL',\n",
       " 'WRB-NC',\n",
       " 'WRB-TL',\n",
       " '``']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_words = sorted(set([tag for (word, tag) in brown.tagged_words()]))\n",
    "list_of_words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which nouns are more common in their Plural form, rather then their singular form? # only consider the words with -s suffix\n",
    "# To DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which word has the greatest number of distinct tags. What are they, and what do they represent?\n",
    "\n",
    "# TO Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 152470),\n",
       " ('IN', 120557),\n",
       " ('AT', 97959),\n",
       " ('JJ', 64028),\n",
       " ('.', 60638),\n",
       " (',', 58156),\n",
       " ('NNS', 55110),\n",
       " ('CC', 37718),\n",
       " ('RB', 36464),\n",
       " ('NP', 34476),\n",
       " ('VB', 33693),\n",
       " ('VBN', 29186),\n",
       " ('VBD', 26167),\n",
       " ('CS', 22143),\n",
       " ('PPS', 18253),\n",
       " ('VBG', 17893),\n",
       " ('PP$', 16872),\n",
       " ('TO', 14918),\n",
       " ('PPSS', 13802),\n",
       " ('CD', 13510)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list tags in order of decreasing frequency. 20 frequent words?\n",
    "\n",
    "brown_tagged = brown.tagged_words()\n",
    "list_freq = [w for (word, w) in brown_tagged]\n",
    "freq_words = nltk.FreqDist(list_freq)\n",
    "freq_words.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IN', '.', ',', 'CC', 'NN', 'NNS', 'VBD', 'CS', 'MD', 'BEZ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which tags are nouns most commonly found after? What do these tags represent?\n",
    "\n",
    "word_tag_pairs = nltk.bigrams(brown_tagged)\n",
    "noun_after = [b[1] for (a, b) in word_tag_pairs if a[1].startswith('NN')]\n",
    "fdist = nltk.FreqDist(noun_after)\n",
    "[t for (t, _) in fdist.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 \n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18 \n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest are sort of hard to understand but once I do I will upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
