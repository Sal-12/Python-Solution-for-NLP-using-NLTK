{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import random \n",
    "import pprint\n",
    "from nltk import word_tokenize\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# I have not typed the questions for this chapter as they are too long and too many. Refer to the text book for questions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colourless\n"
     ]
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 'dishes'\n",
    "b[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'running'\n",
    "a[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nation'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 'nationality'\n",
    "r[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = 'undo'\n",
    "u[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 'preheat'\n",
    "p[3:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 3:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "before starting the string the indexerror that I can think is the adding something that not in the list to the left before the string begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## int['hello']   # int type, and list type together cannot be computed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 'monty python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pto'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[6:11:2]       #[starts:ends:stepsover]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'otp'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[10:5:-2]   # reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mtph'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0:11:3]      #skips 3 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[11:6:-3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nohtyp ytnom'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[::-1] # each word is spelt backwards, the slicing under goes twice resulting the backwards spelling. #m='monty' as I have used"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 6: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# findall - finds all the words that are required to be found and nltk.re_show - shows are the resulting words from the text. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# a. [a-zA-Z]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'we', 'will', 'learn', 'about', 'the', 'Regex']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'[a-zA-Z]+', 'Today we will learn about the Regex'))      # + one or more of the previous item is upper case or lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Today} {we} {will} {learn} {about} {the} {Regex}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'[a-zA-Z]+', 'Today we will learn about the Regex')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# b.[A-Z][a-z]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'Regex']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'[A-Z][a-z]*', 'Today we will study about the Regex'))     # * Zero or more first items , first letter is upper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Today} we will study about the {Regex}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'[A-Z][a-z]*', 'Today we will study about the Regex')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# c.p[aeiou]{,2}t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['put', 'pit', 'pot']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'p[aeiou]{,2}t', 'Pat put perfect pit pot'))   # words starting with p and ending with t with 2 vowels between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pat {put} perfect {pit} {pot}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'p[aeiou]{,2}t', 'Pat put perfect pit pot')    # only one word with pt and no other word is there in the text with [aeiou] in the middle."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# d.\\d+(\\.\\d+)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lucky number {3}, {12}, {22}, surprisingly {3.12}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)?', 'lucky number 3, 12, 22, surprisingly 3.12')  # includes digits in the text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# e.([^aeiou][aeiou][^aeiou])*     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lo ', '', 'her', '', '', 'how', '', 'hav', '', '', '', '', '', '', 've ', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'([^aeiou][aeiou][^aeiou])*', 'hello there how have you\\'ve been'))   #consonant-vowel-consonant with zero or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{hello }{}t{her}{}e{} {how}{} {hav}{}e{} {}y{}o{}u{}'{ve }{}b{}e{}e{}n{}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', 'hello there how have you\\'ve been')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# f.\\w+|[^\\w\\s]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'f.\\w+|[^\\w\\s]+', 'we will study the nlp on 12 \\n'))    # alphanumeric and non-whitespace characters are used for tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will study the nlp on 12\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'f.\\w+|[^\\w\\s]+', 'we will study the nlp on 12 \\n')      #\\n new line does not "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 7 :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 7.a :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = r'\\b(a|an|the)\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An artist works on {the} art for {a} competition\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(a, 'An artist works on the art for a competition')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 7.B :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = r'\\d+([\\+|*]\\d+)+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{234+1223}, {231*23}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(b, '234+1223, 231*23')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 8 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Toolkit', '—', 'NLTK', '3.4.5', 'documentation', 'NLTK', '3.4.5', 'documentation', 'next', '|', 'modules', '|', 'index', 'Natural', 'Language', 'Toolkit¶', 'NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.', 'Thanks', 'to', 'a', 'hands-on', 'guide', 'introducing', 'programming', 'fundamentals', 'alongside', 'topics', 'in', 'computational', 'linguistics', ',', 'plus', 'comprehensive', 'API', 'documentation', ',', 'NLTK', 'is', 'suitable', 'for', 'linguists', ',', 'engineers', ',', 'students', ',', 'educators', ',', 'researchers', ',', 'and', 'industry', 'users', 'alike', '.', 'NLTK', 'is', 'available', 'for', 'Windows', ',', 'Mac', 'OS', 'X', ',', 'and', 'Linux', '.', 'Best', 'of', 'all', ',', 'NLTK', 'is', 'a', 'free', ',', 'open', 'source', ',', 'community-driven', 'project', '.', 'NLTK', 'has', 'been', 'called', '“', 'a', 'wonderful', 'tool', 'for', 'teaching', ',', 'and', 'working', 'in', ',', 'computational', 'linguistics', 'using', 'Python', ',', '”', 'and', '“', 'an', 'amazing', 'library', 'to', 'play', 'with', 'natural', 'language.', '”', 'Natural', 'Language', 'Processing', 'with', 'Python', 'provides', 'a', 'practical', 'introduction', 'to', 'programming', 'for', 'language', 'processing', '.', 'Written', 'by', 'the', 'creators', 'of', 'NLTK', ',', 'it', 'guides', 'the', 'reader', 'through', 'the', 'fundamentals', 'of', 'writing', 'Python', 'programs', ',', 'working', 'with', 'corpora', ',', 'categorizing', 'text', ',', 'analyzing', 'linguistic', 'structure', ',', 'and', 'more', '.', 'The', 'online', 'version', 'of', 'the', 'book', 'has', 'been', 'been', 'updated', 'for', 'Python', '3', 'and', 'NLTK', '3', '.', '(', 'The', 'original', 'Python', '2', 'version', 'is', 'still', 'available', 'at', 'http', ':', '//nltk.org/book_1ed', '.', ')', 'Some', 'simple', 'things', 'you', 'can', 'do', 'with', 'NLTK¶', 'Tokenize', 'and', 'tag', 'some', 'text', ':', '>', '>', '>', 'import', 'nltk', '>', '>', '>', 'sentence', '=', '``', \"''\", \"''\", 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', \"''\", \"''\", \"''\", '>', '>', '>', 'tokens', '=', 'nltk.word_tokenize', '(', 'sentence', ')', '>', '>', '>', 'tokens', '[', \"'At\", \"'\", ',', \"'eight\", \"'\", ',', '``', \"o'clock\", \"''\", ',', \"'on\", \"'\", ',', \"'Thursday\", \"'\", ',', \"'morning\", \"'\", ',', \"'Arthur\", \"'\", ',', \"'did\", \"'\", ',', '``', \"n't\", \"''\", ',', \"'feel\", \"'\", ',', \"'very\", \"'\", ',', \"'good\", \"'\", ',', \"'\", '.', \"'\", ']', '>', '>', '>', 'tagged', '=', 'nltk.pos_tag', '(', 'tokens', ')', '>', '>', '>', 'tagged', '[', '0:6', ']', '[', '(', \"'At\", \"'\", ',', \"'IN\", \"'\", ')', ',', '(', \"'eight\", \"'\", ',', \"'CD\", \"'\", ')', ',', '(', '``', \"o'clock\", \"''\", ',', \"'JJ\", \"'\", ')', ',', '(', \"'on\", \"'\", ',', \"'IN\", \"'\", ')', ',', '(', \"'Thursday\", \"'\", ',', \"'NNP\", \"'\", ')', ',', '(', \"'morning\", \"'\", ',', \"'NN\", \"'\", ')', ']', 'Identify', 'named', 'entities', ':', '>', '>', '>', 'entities', '=', 'nltk.chunk.ne_chunk', '(', 'tagged', ')', '>', '>', '>', 'entities', 'Tree', '(', \"'S\", \"'\", ',', '[', '(', \"'At\", \"'\", ',', \"'IN\", \"'\", ')', ',', '(', \"'eight\", \"'\", ',', \"'CD\", \"'\", ')', ',', '(', '``', \"o'clock\", \"''\", ',', \"'JJ\", \"'\", ')', ',', '(', \"'on\", \"'\", ',', \"'IN\", \"'\", ')', ',', '(', \"'Thursday\", \"'\", ',', \"'NNP\", \"'\", ')', ',', '(', \"'morning\", \"'\", ',', \"'NN\", \"'\", ')', ',', 'Tree', '(', \"'PERSON\", \"'\", ',', '[', '(', \"'Arthur\", \"'\", ',', \"'NNP\", \"'\", ')', ']', ')', ',', '(', \"'did\", \"'\", ',', \"'VBD\", \"'\", ')', ',', '(', '``', \"n't\", \"''\", ',', \"'RB\", \"'\", ')', ',', '(', \"'feel\", \"'\", ',', \"'VB\", \"'\", ')', ',', '(', \"'very\", \"'\", ',', \"'RB\", \"'\", ')', ',', '(', \"'good\", \"'\", ',', \"'JJ\", \"'\", ')', ',', '(', \"'\", '.', \"'\", ',', \"'\", '.', \"'\", ')', ']', ')', 'Display', 'a', 'parse', 'tree', ':', '>', '>', '>', 'from', 'nltk.corpus', 'import', 'treebank', '>', '>', '>', 't', '=', 'treebank.parsed_sents', '(', \"'wsj_0001.mrg\", \"'\", ')', '[', '0', ']', '>', '>', '>', 't.draw', '(', ')', 'NB', '.', 'If', 'you', 'publish', 'work', 'that', 'uses', 'NLTK', ',', 'please', 'cite', 'the', 'NLTK', 'book', 'as', 'follows', ':', 'Bird', ',', 'Steven', ',', 'Edward', 'Loper', 'and', 'Ewan', 'Klein', '(', '2009', ')', ',', 'Natural', 'Language', 'Processing', 'with', 'Python', '.', 'O', '’', 'Reilly', 'Media', 'Inc.', 'Next', 'Steps¶', 'sign', 'up', 'for', 'release', 'announcements', 'join', 'in', 'the', 'discussion', 'Contents¶', 'NLTK', 'News', 'Installing', 'NLTK', 'Installing', 'NLTK', 'Data', 'Contribute', 'to', 'NLTK', 'FAQ', 'Wiki', 'API', 'HOWTO', 'Index', 'Module', 'Index', 'Search', 'Page', 'Table', 'of', 'Contents', 'NLTK', 'News', 'Installing', 'NLTK', 'Installing', 'NLTK', 'Data', 'Contribute', 'to', 'NLTK', 'FAQ', 'Wiki', 'API', 'HOWTO', 'Search', 'next', '|', 'modules', '|', 'index', 'Show', 'Source', '©', 'Copyright', '2019', ',', 'NLTK', 'Project', '.', 'Last', 'updated', 'on', 'Aug', '20', ',', '2019', '.', 'Created', 'using', 'Sphinx', '2.1.2', '.']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "def Contents_of_The_URL(URL):\n",
    "    html_text = request.urlopen(URL).read().decode('utf8')\n",
    "    raw_text = BeautifulSoup(html_text).get_text()\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    return tokens\n",
    "\n",
    "print(Contents_of_The_URL('http://nltk.org/'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 9 :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 9.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    file = open(f)\n",
    "    raw = file.read()\n",
    "    pattern = r'''(?x)[,\\.]|[\\[\\](){}<>]|['\"``]|[?!]|[:;]|\\.\\.\\.|[, . ? ! `: ;] \n",
    "             '''                                                                # Includes all the brackets, commas, inverted commas, ellipsis,dash, periods etc.\n",
    "    return nltk.regexp_tokenize(raw, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', ',', ' ', ' ', ' ', '?', \"'\", ' ', ' ', ' ', '?', ' ', ' ', ' ', ' ', '.', ',', ' ', ' ', ' ', ' ', '\"', '\"', ' ', ' ', '.', '!', ' ', ',', ' ', ',', ' ', ' ', '.', ' ', ':', '.', ':', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '.', '.', ' ', ' ', ' ', ',', ' ', ',', ' ', ',', ' ', ',', ' ', ',', ' ', ',', ' ', ',', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(load('corpus.txt'))    # write a text in the jupyter notebook as a text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 9.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monetary_values(m):\n",
    "    file = open(m)\n",
    "    raw = file.read()\n",
    "    pattern = r'''(?x)\n",
    "              \\$\\d+(?:,\\d+)*(?:\\.\\d+)?     # US dollars\n",
    "              |pound\\d+(?:,\\d+)*(?:\\.\\d+)?  #british pounds, symbols should be used since I could'nt find it i used pound\n",
    "              '''\n",
    "    \n",
    "    return nltk.regexp_tokenize(raw, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$5000', 'pound236.34']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monetary_values('corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12.1.2019', '21/12/2012', '12 December 2019', '20-12-2019']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_of_dates = r'''(?x)\n",
    "                   \\d{2}.\\d{1,2}.\\d{4}|\\d{2}-\\d{2}-\\d{4}|\\d{2}/\\d{2}/\\d{4}|\\d{1,2}\\s[A-Z][a-z]{2,9}\\s\\d{4}\n",
    "                   '''\n",
    "nltk.regexp_tokenize('date is 12.1.2019, 21/12/2012, 12 December 2019, 20-12-2019', pattern_of_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neruda',\n",
       " 'Coelho',\n",
       " 'Musk',\n",
       " 'Spacy',\n",
       " 'Nfosys',\n",
       " 'Jiojio',\n",
       " 'metro',\n",
       " 'the',\n",
       " 'free',\n",
       " 'internet']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_and_organisation = r'''(?x)\n",
    "                          [A-Z][a-z]+(?:\\s[A-Z][a-z]+)?|[a-z]+\n",
    "                          '''\n",
    "n_a_o = 'Neruda, Coelho, Musk, Spacy, INfosys, Jiojio, metro, the free internet'\n",
    "nltk.regexp_tokenize(n_a_o, names_and_organisation) #add things and find out more ways to write it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = [(w, len(w)) for w in sent]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'et the ', 'y', 'tem ', 'traight to the ', 'oftware']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'set the system straight to the software'    # first letter 's' is split from the text.\n",
    "raw.split('s')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "e\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "y\n",
      "s\n",
      "t\n",
      "e\n",
      "m\n",
      " \n",
      "s\n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "o\n",
      "f\n",
      "t\n",
      "w\n",
      "a\n",
      "r\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "for words in raw:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'the tree is to the left of the trampoline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'tree', 'is', 'to', 'the', 'left', 'of', 'the', 'trampoline']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'tree', 'is', 'to', 'the', 'left', 'of', 'the', 'trampoline']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '\\tthe\\ttree\\tis\\tto\\tthe\\tleft\\tof\\tthe\\ttrampoline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'tree', 'is', 'to', 'the', 'left', 'of', 'the', 'trampoline']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tthe\\ttree\\tis\\tto\\tthe\\tleft\\tof\\tthe\\ttrampoline']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '\\tthe  \\ttree  \\tis  \\tto  \\tthe  \\tleft  \\tof  \\tthe  \\ttrampoline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'tree', 'is', 'to', 'the', 'left', 'of', 'the', 'trampoline']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tthe',\n",
       " '\\ttree',\n",
       " '\\tis',\n",
       " '\\tto',\n",
       " '\\tthe',\n",
       " '\\tleft',\n",
       " '\\tof',\n",
       " '\\tthe',\n",
       " '\\ttrampoline']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split('  ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tthe',\n",
       " '',\n",
       " '\\ttree',\n",
       " '',\n",
       " '\\tis',\n",
       " '',\n",
       " '\\tto',\n",
       " '',\n",
       " '\\tthe',\n",
       " '',\n",
       " '\\tleft',\n",
       " '',\n",
       " '\\tof',\n",
       " '',\n",
       " '\\tthe',\n",
       " '',\n",
       " '\\ttrampoline']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set', 'the', 'system', 'straight', 'to', 'the', 'software']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set', 'the', 'system', 'straight', 'to', 'the', 'software']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent.split() and sent.split(' ') without the \\t have the same result, however the \\t is diplayed in result along with sent in sent.split(' ')."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['set', 'the', 'system', 'straight', 'to', 'the', 'software']\n",
      "['set', 'software', 'straight', 'system', 'the', 'the', 'to']\n"
     ]
    }
   ],
   "source": [
    "words = raw.split()\n",
    "print(words)\n",
    "words.sort()       #words.sort() changes the original list and the result is not default\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['set', 'the', 'system', 'straight', 'to', 'the', 'software']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['set', 'software', 'straight', 'system', 'the', 'the', 'to']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = raw.split()\n",
    "print(words)\n",
    "sorted(words)    # sorted(words) returns a sorted list without changing the original list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set', 'the', 'system', 'straight', 'to', 'the', 'software']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 7   #string '3' should be returned 7 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7 # int value as 3 and 7 are multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\") * 7   #adding int before () makes the mentioned number an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7    #adding str before () makes the mentioned number a string"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prog import monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog.monty"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'processing raw text in nlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing raw text in nlp\n"
     ]
    }
   ],
   "source": [
    "print('%6s' %string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing raw text in nlp\n"
     ]
    }
   ],
   "source": [
    "print('%-6s' %string)    # there is no difference here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processing', 'raw', 'text', 'in', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "string2 = ['processing', 'raw', 'text', 'in', 'nlp']\n",
    "print('%6s' %string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processing', 'raw', 'text', 'in', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "print('%-6s' %string2)      # there is no difference here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(string2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# There is no difference whether the text is in the format of string or list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['why', 'what', 'Whatever', 'whenever', 'whatever', 'Whom', 'Who', 'Where']\n"
     ]
    }
   ],
   "source": [
    "t = 'corpus.txt'\n",
    "file = open(t)\n",
    "raw = file.read()\n",
    "tokens = nltk.wordpunct_tokenize(raw)\n",
    "print([w for w in tokens if w.startswith('wh') or w.startswith('Wh')])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = open('freq_words.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuzzy 53\\n', 'wild 45\\n', 'coke 12\\n', 'test 70\\n', 'tar 766']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words = [[line.split()[0], int(line.split()[1])] for line in corp]   # [0] first value, [1] second value int from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuzzy', 53], ['wild', 45], ['coke', 12], ['test', 70], ['tar', 766]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 20 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# I tried this with many different codes but I was not able to run it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 21"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def unknown(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    lowercase_words = re.findall(r'\\b[a-z]+', html)\n",
    "    unknowns = [w for w in lowercase_words if w not in nltk.corpus.words.words()]\n",
    "    return unknowns\n",
    "\n",
    "unknown('https://en.wikipedia.org/wiki/Main_Page')    # this takes alot of time thus i didn't run it"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(unknown('http://news.bbc.co.uk/'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def unknown(url):\n",
    "    text = urllib.urlopen(url).read()\n",
    "    text = re.sub(r'\\<script(?:.|\\n)*?\\<\\/script\\>', '', text)\n",
    "    text = re.sub(r'\\style(?:.|\\n)*?\\<\\/style\\>', '', text)\n",
    "    raw = BeautifulSoup(text)\n",
    "    contents = raw.get_text()\n",
    "    lowers = re.findall(r'[\\s\\(\\[\\{]([a-z]+)', contents)\n",
    "    words = nltk.corpus.words.words()\n",
    "    return set([w for w in lowers if w not in words])\n",
    "\n",
    "print(unknown('http://www.bbc.com/news'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"don't\", 'like', 'it', 'I', \"won't\", 'do', 'it']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I don't like it, I won't do it\"\n",
    "token = r\"\\w+(?:'t)?\"\n",
    "nltk.regexp_tokenize(text, token)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I 8 an app|3 1n fr0nt 0f my t3a[h3r, $h3 $a1d t0 [a|| my par3nt5 0r Guard1an55w33!5w33!'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I ate an apple in front of my teacher, she said to call my parents or guardians..'\n",
    "\n",
    "text = re.sub(r'ate', '8', text)\n",
    "text = re.sub(r'e', '3', text)\n",
    "text = re.sub(r'i', '1', text)\n",
    "text = re.sub(r'o', '0', text)\n",
    "text = re.sub(r'l', '|', text)\n",
    "text = re.sub(r'\\.', '5w33!', text)\n",
    "\n",
    "text = re.sub(r'(\\b)(s)', r'\\1$', text)\n",
    "text = re.sub(r'(\\w)(s)', r'\\g<1>5', text)\n",
    "\n",
    "text = re.sub(r'g', 'G', text)\n",
    "text = re.sub(r'c', '[', text)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hAck3r'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'hacker'\n",
    "\n",
    "word = re.sub(r'a', 'A', word)\n",
    "word = re.sub(r'e', '3', word)\n",
    "\n",
    "word"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 25"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 25 . a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingstray'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pig_latin(word):\n",
    "    cons_pattern = re.findall(r'^[^aeiouAEIOU]*', word)\n",
    "    return word[len(cons_pattern[0]):] + cons_pattern[0] +'ay'\n",
    "\n",
    "pig_latin('string')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 25 . b and c are not giving the required results I will try to slow the issue with my kernel, unfortunately it keeps dying."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Removed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-4cf5ba3f22ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRemoved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Removed' is not defined"
     ]
    }
   ],
   "source": [
    "#Removed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaaeehhhhehaheaaaheheaaheaehheheehheaaaeehheeheheaahhhhehaahehahhaeeeheeeaaehhheeehhaehhaahhahhaeahhhehhhhaaahheehaheheahhehhhhehhaehahhaaheehhahhheahaaheahhhheeehheheheheahhaheehahehaehheeaaeheheahhaheehhehehhahhahhheheahhehhehaheaheaeehhhaahaeeahhhheehhehheeaeeeahhhaaaehhhahhhhaaehhehahhahaahhhahhahhhhheheaaheahhahhhahehahaheeehehhaahehahheehaaaaeahahhheeaahahehehhhhheaaehhaeeeeahhaaaaaaehaahehhahhhehhhhhhaehehhahheaahhhheheahhahahheheeheaehhhhehhehhhhaehheaehhheheaaaaahahheahaeaehhahhhhhahhhe'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def laughter():\n",
    "    raw = ''.join(random.choice(\"aehh\") for w in range(500))\n",
    "    return ' '.join(raw.split())\n",
    "laughter()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 28"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Nine words should probably be compatible as per the phonetics and relevant to the speech processing applications"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 4.0841684990890705\n",
      "belles_lettres 10.987652885621749\n",
      "editorial 9.471025332953673\n",
      "fiction 4.9104735321302115\n",
      "government 12.08430349501021\n",
      "hobbies 8.922356393630267\n",
      "humor 7.887805248319808\n",
      "learned 11.926007043317348\n",
      "lore 10.254756197101155\n",
      "mystery 3.8335518942055167\n",
      "news 10.176684595052684\n",
      "religion 10.203109907301261\n",
      "reviews 10.769699888473433\n",
      "romance 4.34922419804213\n",
      "science_fiction 4.978058336905399\n"
     ]
    }
   ],
   "source": [
    "def ari(category):\n",
    "    words = nltk.corpus.brown.words(categories=category)\n",
    "    sents = nltk.corpus.brown.sents(categories=category)\n",
    "    avg_word_len = sum(len(w) for w in words) / len(words)\n",
    "    avg_sent_len = sum(len(s) for s in sents) / len(sents)\n",
    "    return (4.71 * avg_word_len) + (0.5 * avg_sent_len) - 21.43\n",
    "\n",
    "for category in brown.categories():\n",
    "    print(category, ari(category))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'histori', 'is', 'complic', ',', 'scienc', 'is', 'good', 'but', 'too', 'mani', 'theori', ',', 'sociolog', 'is', 'bore', 'and', 'math', 'is', 'difficult', ',', 'art', 'is', 'great', 'but', 'everyon', 'is', 'an', 'artist', 'these', 'day', 'thank', 'to', 'instagram', ',', 'musician', 'is', 'an', 'unpredict', 'futur', '.']\n",
      "\n",
      "\n",
      "['the', 'hist', 'is', 'comply', ',', 'sci', 'is', 'good', 'but', 'too', 'many', 'the', ',', 'sociolog', 'is', 'bor', 'and', 'math', 'is', 'difficult', ',', 'art', 'is', 'gre', 'but', 'everyon', 'is', 'an', 'art', 'thes', 'day', 'thank', 'to', 'instagram', ',', 'mus', 'is', 'an', 'unpredict', 'fut', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"The history is complicated, Science is good but too many theories, sociology is boring and maths is difficult,\n",
    "         Art is great but everyone is an artist these days thanks to instagram, Musician is an unpredictable future.\"\"\" # random sent\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "tokens = word_tokenize(raw)\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print(\"\\n\")\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter uses longer stems, uses unicode."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_var = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "length = []\n",
    "for i in list_var:\n",
    "    length.append(len(i))\n",
    "length"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way.']\n"
     ]
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way.'\n",
    "bland = silly.split()\n",
    "print(bland)   # a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(w[1] for w in bland)  #b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(bland)   #c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way.\n"
     ]
    }
   ],
   "source": [
    "for w in sorted(bland):\n",
    "    print(w)       #d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'inexpressible'.index('re') #a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['a', 'list', 'of', 'words']\n",
    "words.index('of')     #b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = bland[:bland.index('in')]  \n",
    "phrase              #c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nationality(adjective):\n",
    "    if (adjective.endswith('dian') or adjective.endswith('ese')):\n",
    "        return adjective[:-3] + 'a'\n",
    "    elif (adjective.endswith('ian')):     # for correct nationalities throughout the world the code will be longer\n",
    "        return adjective[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada\n"
     ]
    }
   ],
   "source": [
    "print(nationality('Canadian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n"
     ]
    }
   ],
   "source": [
    "print(nationality('Chinese'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as best I can', 'as best as I can', 'As best as she can']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronoun_text = \"\"\" I wil straight dispose, as best I can, th'inferiour Magistrate ...\n",
    "And I haue thrust my selfe into this maze, Happily to wiue and thriue, as best I may ...\n",
    "In fine, my life is that of a great schoolboy, getting into scrapes for the fun of it,\n",
    "and fighting my way out as best as I can!\n",
    "As best as she can she hides herself in the full sunlight\n",
    "\"\"\"\n",
    "re.findall(r'(?i)as best (?:as )?(?:I|we|you|he|she|they|it) can', pronoun_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh hai . In teh beginnin Ceiling Cat maded teh skiez An da Urfs , but he did not eated dem . Da Urfs no had shapez An haded dark face , An Ceiling Cat rode invisible bike over teh waterz . An Ceiling Cat sayed light Day An dark no Day . It were FURST !!! 1 An Ceiling Cat sayed , i can has teh firmmint wich iz funny bibel naim 4 ceiling , so wuz teh twoth day . An Ceiling Cat called no waterz urth and waters oshun . Iz good . An so teh threeth day jazzhands . An so teh furth day w00t . An so teh ... fith day . Ceiling Cat taek a wile 2 cawnt . An Ceiling Cat doed moar living stuff , mooes , An creepies , An otehr animuls , An did not eated tehm . An Ceiling Cat sayed , letz us do peeps like uz , becuz we ish teh qte , An let min p0wnz0r becuz tehy has can openers . So Ceiling Cat createded teh peeps taht waz like him , can has can openers he maed tehm , min An womin wuz maeded , but he did not eated tehm . An Ceiling Cat sed them O hai maek bebehs kthx , An p0wn teh waterz , no waterz An teh firmmint , An evry stufs . For evry createded stufs tehre are the fuudz , to the burdies , teh creepiez , An teh mooes , so tehre . It happen . Iz good . An Ceiling Cat sayed , Beholdt , teh good enouf for releaze as version 0 . 8a . kthxbai . An teh skyz an teh Urfs wur finishd , an al teh stufz in dem , an Ceiling Cat was liek al tired an stuf . Ceiling Cat blesd teh 7f day , an sed itz teh h0liez0rz ; cuz dats when he restd fum all his werk wich Ceiling Cat creatd an maed . Yay holy Caturday ! Iz how teh skyz an Urfs wur maed , wen Ceiling Cat pwnt . Urfs no can has plantz n treez n catnipz yet , cuz Ceiling Cat no can maek rainz , but iz ok for kittehs DUNT LYKEZ wetfurz . An ther wuznt ne man to mek farmz n stuf ; cuz teh clowds wur al happie an dint feel liek cryin , wich wuz ok to cuz umbrellaz wuznt inventd yut . An Ceiling Cat madez kitteh owt ov teh flore dust , an breathd ntew his nawstrils teh bref ov life , wich wuz sorta liek doin cpr on a mudpie , but it wuz al gud . An Ceiling Cat madez evry tre dat iz prity , an gud fur fud ; teh tre ov lief wuz in teh gardun to , an teh tre ov teh nawlej ov gud an evul . man askd Ceiling Cat to makez a kooki tree ,\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(nltk.corpus.genesis.words('lolcat.txt')[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'siet kiet dood ovah kitteh littel'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.corpus.genesis.words('lolcat.txt')\n",
    "text = 'sight kite dude over kitty little'\n",
    "# just implement some easy-to-check rules\n",
    "text = re.sub(r'ight', 'iet', text)                             # ight -> iet\n",
    "text = re.sub(r'\\bdude\\b', 'dood', text)                        # dude -> dood\n",
    "text = re.sub(r'([b-df-hj-np-tv-z])(e)\\b', r'\\2\\1', text)       # exchange the consonant and the endding 'e'\n",
    "text = re.sub(r'er\\b', 'ah', text)                              # -er -> -ah\n",
    "text = re.sub(r'y\\b', 'eh', text)                               # -y -> -eh\n",
    "text = re.sub(r'le\\b', 'el', text)                              # -le -> -el\n",
    "text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the Match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A span which should be cleaned'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_cleaner(html):\n",
    "    text = re.sub('\\<.*?\\>', '', html)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "text_cleaner('<span class=\"some class\">A span      which should<br>   be cleaned</span>')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ent-\\nered', 'and-\\nhere']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a\n",
    "text = \"\"\"Text should be ent-\\nered and-\\nhere\"\"\"\n",
    "pattern = r'\\w+-\\n\\w+'\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "e\n",
      "x\n",
      "t\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "-\n",
      "\n",
      "e\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      "-\n",
      "\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "# b\n",
    "for w in text:\n",
    "    print(re.sub('\\n', '', w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text should be ent-ered and-here'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c \n",
    "pattern = r'(\\w+-)(\\n)(\\w+)'\n",
    "re.findall(pattern, text)\n",
    "re.sub(pattern, r'\\1\\3', text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A626'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://en.wikipedia.org/wiki/Soundex\n",
    "# cumbersome implementation...\n",
    "def soundex(word):\n",
    "    word = word.upper()         # convert the word to upper case for convenience\n",
    "    \n",
    "    # Step 1: Retain the first letter\n",
    "    sound = word[0]\n",
    "\n",
    "    # Step 3: If two or more letters with the same number are adjacent \n",
    "    # in the original name (before step 1), only retain the first letter;\n",
    "    word = re.sub(r'([BFPV])[BFPV]', r'\\1', word)             # \n",
    "    word = re.sub(r'([CGJKQSXZ])[CGJKQSXZ]', r'\\1', word)\n",
    "    word = re.sub(r'([DT])[DT]', r'\\1', word)\n",
    "    word = re.sub(r'LL', r'L', word)\n",
    "    word = re.sub(r'([MN])[MN]', r'\\1', word)\n",
    "    word = re.sub(r'RR', r'R', word)\n",
    "    \n",
    "    # Step 3:  two letters with the same number separated by 'h' or 'w' are coded as a single number\n",
    "    word = re.sub(r'([BFPV])([HW])[BFPV]', r'\\1\\2', word)\n",
    "    word = re.sub(r'([CGJKQSXZ])([HW])[CGJKQSXZ]', r'\\1\\2', word)\n",
    "    word = re.sub(r'([DT])([HW])[DT]', r'\\1\\2', word)\n",
    "    word = re.sub(r'L([HW])L', r'L\\1', word)\n",
    "    word = re.sub(r'([MN])([HW])[MN]', r'\\1\\2', word)\n",
    "    word = re.sub(r'R([HW])R', r'R\\1', word)\n",
    "    \n",
    "    # Replace consonants with digits as follows (after the first letter)\n",
    "    word = re.sub(r'[AEIOUYHW]', r'', word)\n",
    "    word = re.sub(r'[BFPV]', '1', word)\n",
    "    word = re.sub(r'[CGJKQSXZ]', '2', word)\n",
    "    word = re.sub(r'[DT]', '3', word)\n",
    "    word = re.sub(r'L', '4', word)\n",
    "    word = re.sub(r'[MN]', '5', word)\n",
    "    word = re.sub(r'R', '6', word)\n",
    "    \n",
    "    # Step 4: If you have too few letters in your word that you can't assign three numbers, \n",
    "    # append with zeros until there are three numbers. If you have more than 3 letters, \n",
    "    # just retain the first 3 numbers.\n",
    "    if sound in 'AEIOUYHW':\n",
    "        sound = (sound + word + '000')[:4]\n",
    "    else:\n",
    "        sound = (sound + word[1:] + '000')[:4]\n",
    "    return sound\n",
    "\n",
    "soundex('Aircraft')     #credit to HaelChan"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.56965469747922\n",
      "68.81483530728246\n"
     ]
    }
   ],
   "source": [
    "def ari(raw):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sents = sent_tokenizer.tokenize(raw)\n",
    "    words = nltk.word_tokenize(raw)\n",
    "    avg_word_length = sum(len(w) for w in words) / len(words)\n",
    "    avg_sent_length = sum(len(s) for s in sents) / len(sents)\n",
    "    return (4.71 * avg_word_length) + (0.5 * avg_sent_length) - 21.43\n",
    "print(ari(nltk.corpus.abc.raw('rural.txt')))\n",
    "print(ari(nltk.corpus.abc.raw('science.txt')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouia', 'eouio', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution', 'seqouia', 'tenacious', 'unidirectional']\n",
    "v_seq = [''.join(re.findall(r'[aeiou]', v)) for v in words]\n",
    "sorted(v_seq)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        # self._index = nltk.Index((self._stem(word), i)\n",
    "        #                          for (i, word) in enumerate(text))\n",
    "        self._index = nltk.Index((wn.synsets(self._stem(word))[0].offset(), i)\n",
    "                                 for (i, word) in enumerate(text) \n",
    "                                 if wn.synsets(self._stem(word)) != [])     # to avoid list index out of range\n",
    "        \n",
    "        # basic idea: use WordNet's offset as the word's key rather than the word itself\n",
    "        \n",
    "    def concordance(self, word, width=40):\n",
    "        key = wn.synsets(self._stem(word))[0].offset()\n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "    \n",
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('begin')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French_Francais-Latin1\n",
      "German_Deutsch-Latin1\n",
      "Italian-Latin1\n"
     ]
    }
   ],
   "source": [
    "def guess_language(text):\n",
    "    candidate_language = ['French_Francais-Latin1', 'German_Deutsch-Latin1', 'Italian-Latin1']\n",
    "\n",
    "    fdist = nltk.FreqDist(lang for lang in candidate_language\n",
    "                               for w in text if w in nltk.corpus.udhr.words(lang))\n",
    "    return fdist\n",
    "text_french = \"Wikipédia Écouter est un projet d'encyclopédie universelle, multilingue, créé par Jimmy Wales et Larry Sanger le 15 janvier 2001 en wiki sous le nom de domaine wikipedia.org. Les versions des différentes langues utilisent le même logiciel de publication, MediaWiki, et ont la même apparence, mais elles comportent des variations dans leurs contenus, leurs structures et leurs modalités d'édition et de gestion.\".split()\n",
    "text_german = \"Wikipedia ist ein am 15. Januar 2001 gegründetes gemeinnütziges Projekt zur Erstellung einer Enzyklopädie in zahlreichen Sprachen mit Hilfe des Wiki­prinzips. Gemäß Publikumsnachfrage und Verbreitung gehört Wikipedia unterdessen zu den Massenmedien. Aufgrund der für die Entstehung und Weiterentwicklung dieser Enzyklopädie charakteristischen kollaborativen Erstellungs-, Kontroll- und Aushandlungsprozesse der ehrenamtlichen Beteiligten zählt Wikipedia zugleich zu den Social Media.\".split()\n",
    "text_italian = \"Wikipedia (pronuncia: vedi sotto) è un'enciclopedia online a contenuto libero, collaborativa, multilingue e gratuita, nata nel 2001, sostenuta e ospitata dalla Wikimedia Foundation, un'organizzazione non a scopo di lucro statunitense. Lanciata da Jimmy Wales e Larry Sanger il 15 gennaio 2001, inizialmente nell'edizione in lingua inglese, nei mesi successivi ha aggiunto edizioni in numerose altre lingue. Sanger ne suggerì il nome,[1] una parola macedonia nata dall'unione della radice wiki al suffisso pedia (da enciclopedia).\".split()\n",
    "print(guess_language(text_french).max())\n",
    "print(guess_language(text_german).max())\n",
    "print(guess_language(text_italian).max())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will work on it when my kernel isn't dying constantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read the paper if you want or don't..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### This Chapter is Completed Sort of...####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
